{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random \n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images=np.load('data/images.npy')\n",
    "labels=np.load('data/labels.npy')\n",
    "labels=labels.astype(int)\n",
    "labels[labels!=0]=-1\n",
    "labels[labels==0]=1\n",
    "labels[labels==-1]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10d284b38>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13203fba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "apples=images[labels==1]\n",
    "print(len(apples))\n",
    "plt.imshow(apples[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_images_flat=(images.shape[0],images.shape[1]*images.shape[2])\n",
    "images_flat=np.ndarray(shape=shape_images_flat)\n",
    "for index in range(len(images)):\n",
    "    images_flat[index]=images[index].flat\n",
    "images_flat=(images_flat-images_flat.mean())/images.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images=images_flat[0:40000]\n",
    "train_labels=labels[0:40000]\n",
    "\n",
    "validation_images=images_flat[40000:45000]\n",
    "validation_labels=labels[40000:45000]\n",
    "\n",
    "test_images=images_flat[45000:50000]\n",
    "test_labels=labels[45000:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):  \n",
    "    return math.exp(-np.logaddexp(0, -x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_2(y, y_hat):\n",
    "    \"\"\"Compute accuracy.\n",
    "    Args:\n",
    "    y: A 1-D int NumPy array.\n",
    "    y_hat: A 1-D int NumPy array.\n",
    "    Returns:\n",
    "    A float, the fraction of time y[i] == y_hat[i].\n",
    "    \"\"\"\n",
    "   \n",
    "    a=(y==y_hat)\n",
    "    return a.astype(np.float).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_nn(images,\\\n",
    "            input_weights,input_bias_weight,\\\n",
    "            hidden_weights,hidden_bias_weight):\n",
    "    \n",
    "    images=np.asmatrix(images)\n",
    "    input_weights=np.asmatrix(input_weights)\n",
    "    hidden_weights=np.asmatrix(hidden_weights)\n",
    "    \n",
    "    first_layer=images*input_weights.T\n",
    "    # first_layer+=input_bias_weight\n",
    "    \n",
    "    vfunc = np.vectorize(sigmoid)\n",
    "    \n",
    "    output=vfunc(first_layer)*hidden_weights\n",
    "    # hidden_bias_weight\n",
    "    output[output>0]=1\n",
    "    output[output<=0]=0\n",
    "    return output\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_accuracy_list=[]\n",
    "validation_accuracy_list=[]\n",
    "def compute_accuracy_graph(input_weights,input_bias_weight,hidden_weights,hidden_bias_weight):\n",
    "    train_images_len=len(train_images)\n",
    "    train_shuffler_list=list(range(0, train_images_len))\n",
    "    random.shuffle(train_shuffler_list)    \n",
    "    shuffled_train_images=train_images[train_shuffler_list]\n",
    "    shuffled_train_labels=train_labels[train_shuffler_list]\n",
    "    \n",
    "    shuffled_train_images_used=shuffled_train_images[0:1000]\n",
    "    shuffled_train_labels_used=shuffled_train_labels[0:1000]\n",
    "    \n",
    "    #d_train=shuffled_train_images_used.dot(W)\n",
    "    d_train = predict_nn(shuffled_train_images_used,\\\n",
    "            input_weights,input_bias_weight,\\\n",
    "            hidden_weights,hidden_bias_weight)\n",
    "    \n",
    "    \n",
    "    ac_train=accuracy(shuffled_train_labels_used,d_train)\n",
    "    training_accuracy_list.append(ac_train)\n",
    "    \n",
    "\n",
    "    validation_images_len=len(validation_images)\n",
    "    validation_shuffler_list=list(range(0, validation_images_len))\n",
    "    random.shuffle(validation_shuffler_list)    \n",
    "    shuffled_validation_images=validation_images[validation_shuffler_list]\n",
    "    shuffled_validation_labels=validation_labels[validation_shuffler_list]  \n",
    "    \n",
    "    shuffled_validation_images_used=shuffled_validation_images[0:5000]\n",
    "    shuffled_validation_labels_used=shuffled_validation_labels[0:5000]\n",
    "    \n",
    "    #d_validation=shuffled_validation_images_used.dot(W)\n",
    "    \n",
    "    d_validation = predict_nn(shuffled_validation_images_used,\\\n",
    "            input_weights,input_bias_weight,\\\n",
    "            hidden_weights,hidden_bias_weight)\n",
    "    \n",
    "    ac_validation=accuracy(shuffled_validation_labels_used,d_validation)\n",
    "    validation_accuracy_list.append(ac_validation)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I=1\n",
    "learning_rate=.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.7489\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-0.6137\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-0.9134\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1690\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-0.3608\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X=train_images\n",
    "Y=train_labels\n",
    "number_of_images=X.shape[0]\n",
    "number_of_hidden_units=30\n",
    "\n",
    "W_tensor=torch.randn(number_of_hidden_units,X.shape[1]).double()\n",
    "input_bias_weight=torch.randn(1).double()\n",
    "\n",
    "# make input bias and input weigths variables\n",
    "input_bias_weight=Variable(0.2*input_bias_weight,requires_grad=True)\n",
    "w_1_tensor=Variable(0.2*W_tensor,requires_grad=True)\n",
    "\n",
    "w_2_tensor=torch.randn(number_of_hidden_units,1).double()\n",
    "hidden_bias_weight=torch.randn(1).double()\n",
    "# make hidden bias and hidden weigths variables\n",
    "hidden_bias_weight=Variable(0.2*hidden_bias_weight,requires_grad=True)\n",
    "w_2_tensor=Variable(0.2*w_2_tensor,requires_grad=True)\n",
    "\n",
    "# print(bias_weight)\n",
    "# print(W_tensor[0])\n",
    "# print(W_tensor.size())\n",
    "\n",
    "for epoch in range(0,I):   \n",
    "    for i in range(0,number_of_images):\n",
    "        \n",
    "        #forward propogation part\n",
    "        x_tensor=torch.from_numpy(X[i])\n",
    "        x_tensor=torch.unsqueeze(x_tensor, 0)\n",
    "        x_tensor=Variable(x_tensor,requires_grad=False)\n",
    "        \n",
    "        # w_1_x=torch.mm(w_1_tensor,x_tensor.t())+input_bias_weight\n",
    "        # w_1_x=torch.sigmoid(w_1_x)\n",
    "        \n",
    "        w2_w1_x=torch.mm(torch.tanh(x_tensor.mm(torch.transpose(w_1_tensor, 0, 1))),w_2_tensor)\n",
    "        prob=torch.sigmoid(w2_w1_x)\n",
    "        \n",
    "        esp1, esp2 = 1e-2,1e2\n",
    "        J=(float(Y[i]))*torch.log(prob.clamp(esp1,esp2))+(1-float(Y[i]))*torch.log((1-prob).clamp(esp1,esp2))\n",
    "        J.backward()\n",
    "\n",
    "        #back propogation part\n",
    "        w_1_tensor.data += learning_rate * w_1_tensor.grad.data\n",
    "        # input_bias_weight.data += learning_rate * input_bias_weight.grad.data\n",
    "\n",
    "        w_2_tensor.data += learning_rate * w_2_tensor.grad.data\n",
    "        # hidden_bias_weight.data += learning_rate * hidden_bias_weight.grad.data\n",
    "        \n",
    "        if(i%100==0):\n",
    "            compute_accuracy_graph(w_1_tensor.data.numpy(),input_bias_weight.data.numpy(),\\\n",
    "                                   w_2_tensor.data.numpy(),hidden_bias_weight.data.numpy())\n",
    "        if(i%1000==0):  \n",
    "            print(J)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        # input_bias_weight.grad.data.zero_()\n",
    "        w_1_tensor.grad.data.zero_()\n",
    "        w_2_tensor.grad.data.zero_()\n",
    "        # hidden_bias_weight.grad.data.zero_()\n",
    "        \n",
    "        #shape sanity checks\n",
    "        # print(w_x.size())\n",
    "        # print(a_w_x.size())\n",
    "    print(J)  \n",
    "\n",
    "weights=(w_1_tensor.data.numpy(),input_bias_weight.data.numpy(),\\\n",
    "                                   w_2_tensor.data.numpy(),hidden_bias_weight.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "-1.1384\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X=train_images\n",
    "Y=train_labels\n",
    "number_of_images=X.shape[0]\n",
    "number_of_hidden_units=30\n",
    "\n",
    "W_tensor=torch.randn(number_of_hidden_units,X.shape[1]).double()\n",
    "input_bias_weight=torch.randn(1).double()\n",
    "\n",
    "# make input bias and input weigths variables\n",
    "input_bias_weight=Variable(0.2*input_bias_weight,requires_grad=True)\n",
    "w_1_tensor=Variable(0.2*W_tensor,requires_grad=True)\n",
    "\n",
    "w_2_tensor=torch.randn(number_of_hidden_units,1).double()\n",
    "hidden_bias_weight=torch.randn(1).double()\n",
    "# make hidden bias and hidden weigths variables\n",
    "hidden_bias_weight=Variable(0.2*hidden_bias_weight,requires_grad=True)\n",
    "w_2_tensor=Variable(0.2*w_2_tensor,requires_grad=True)\n",
    "\n",
    "# print(bias_weight)\n",
    "# print(W_tensor[0])\n",
    "# print(W_tensor.size())\n",
    "\n",
    "for epoch in range(0,I):   \n",
    "    for i in range(0,number_of_images):\n",
    "        \n",
    "        #forward propogation part\n",
    "        x_tensor=torch.from_numpy(X[i])\n",
    "        x_tensor=torch.unsqueeze(x_tensor, 0)\n",
    "        x_tensor=Variable(x_tensor,requires_grad=False)\n",
    "        \n",
    "        mid_layer = torch.tanh(x_tensor.view(1,-1).mm(torch.transpose(w_1_tensor,0,1)))\n",
    "        out_layer = torch.sigmoid(mid_layer.mm(w_2_tensor))\n",
    "            \n",
    "        eps1, eps2 = 1e-5,1e5\n",
    "        loss = (float(Y[i]) * (out_layer.clamp(eps1, eps2).log())) + \\\n",
    "                (1-float(Y[i]) * ((1-out_layer).clamp(eps1, eps2).log()))  \n",
    "        loss.backward()\n",
    " \n",
    "        lr1 = learning_rate * w_1_tensor.grad.data\n",
    "        lr2 = learning_rate * w_2_tensor.grad.data\n",
    "            \n",
    "        w_1_tensor.data = w_1_tensor.data + lr1\n",
    "        w_2_tensor.data = w_2_tensor.data + lr2\n",
    "            \n",
    "        w_2_tensor.grad.data.zero_()\n",
    "        w_1_tensor.grad.data.zero_()        \n",
    "        if(i%100==0):\n",
    "            compute_accuracy_graph(w_1_tensor.data.numpy(),input_bias_weight.data.numpy(),\\\n",
    "                                   w_2_tensor.data.numpy(),hidden_bias_weight.data.numpy())\n",
    "        if(i%1000==0):  \n",
    "            print(J)\n",
    "        # input_bias_weight.grad.data.zero_()\n",
    "        w_1_tensor.grad.data.zero_()\n",
    "        w_2_tensor.grad.data.zero_()\n",
    "        # hidden_bias_weight.grad.data.zero_()\n",
    "        \n",
    "        #shape sanity checks\n",
    "        # print(w_x.size())\n",
    "        # print(a_w_x.size())\n",
    "    print(J)  \n",
    "\n",
    "weights=(w_1_tensor.data.numpy(),input_bias_weight.data.numpy(),\\\n",
    "                                   w_2_tensor.data.numpy(),hidden_bias_weight.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 676)\n",
      "(40000,)\n"
     ]
    }
   ],
   "source": [
    "X=train_images\n",
    "Y=train_labels\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "number_of_images=X.shape[0]\n",
    "hidden_units=30\n",
    "output_nodes=1\n",
    "m=X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-2b21de903bde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0meps1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1e2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mout_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensor_env/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mclamp\u001b[0;34m(self, min, max)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mCmaxConstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mClamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreciprocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensor_env/lib/python3.6/site-packages/torch/autograd/_functions/pointwise.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, min_val, max_val)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w1 = Variable((0.2*torch.randn(X.shape[1],hidden_units)).double(),requires_grad=True) # m x P\n",
    "w2 = Variable((0.2*torch.randn(hidden_units,output_nodes)).double(),requires_grad=True) # P x output\n",
    "    \n",
    "m1_ = Variable((torch.zeros((m, hidden_units)).double()))\n",
    "v1 = Variable((torch.zeros((m, hidden_units)).double()))\n",
    "    \n",
    "m2_ = Variable((torch.zeros((hidden_units, output_nodes)).double()))\n",
    "v2 = Variable((torch.zeros((hidden_units, output_nodes)).double()))\n",
    "    \n",
    "t = 0\n",
    "n=number_of_images\n",
    "for iterations in range(1):\n",
    "    for i in range(n):\n",
    "        x_tensor=torch.from_numpy(X[i])\n",
    "        x_tensor=torch.unsqueeze(x_tensor, 0)\n",
    "        x_tensor=Variable(x_tensor,requires_grad=False)\n",
    "        \n",
    "    #   mid_layer = X_train[i].view(1,-1).mm(w1).clamp(min=0)\n",
    "        mid_layer = torch.tanh(x_tensor.mm(w1))\n",
    "        out_layer = torch.sigmoid(mid_layer.mm(w2))\n",
    "            \n",
    "        eps1, eps2 = 1e-2,1e2\n",
    "        loss = (float(Y[i]) * (out_layer.clamp(eps1, eps2).log())) + \\\n",
    "                (1-float(Y[i])) * ((1-out_layer).clamp(eps1, eps2).log())  \n",
    "        loss.backward()\n",
    " \n",
    "        lr1 = learning_rate * w1.grad.data\n",
    "        lr2 = learning_rate * w2.grad.data\n",
    "            \n",
    "        w1.data = w1.data + lr1\n",
    "        w2.data = w2.data + lr2\n",
    "            \n",
    "        w2.grad.data.zero_()\n",
    "        w1.grad.data.zero_()\n",
    "\n",
    "    print(loss.data)\n",
    "weights=(w1.data.numpy(),1,w2.data.numpy(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=train_images.shape[0]\n",
    "hidden_layers=10\n",
    "output_nodes=1\n",
    "m=train_images.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-0.1676\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "\n",
      "-0.1695\n",
      "[torch.DoubleTensor of size 1x1]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prt code\n",
    "X_train = Variable(torch.from_numpy(train_images)).double()\n",
    "y_train = Variable(torch.from_numpy(train_labels).view(-1,1)).double()\n",
    "for super in range(1):\n",
    "    w1 = Variable((0.2*torch.randn(m,hidden_layers)).double(),requires_grad=True) # m x P\n",
    "    w2 = Variable((0.2*torch.randn(hidden_layers,output_nodes)).double(),requires_grad=True) # P x output\n",
    "    \n",
    "    m1_ = Variable((torch.zeros((m, hidden_layers)).double()))\n",
    "    v1 = Variable((torch.zeros((m, hidden_layers)).double()))\n",
    "    \n",
    "    m2_ = Variable((torch.zeros((hidden_layers, output_nodes)).double()))\n",
    "    v2 = Variable((torch.zeros((hidden_layers, output_nodes)).double()))\n",
    "    t = 0\n",
    "    for iterations in range(2):\n",
    "        for i in range(n):\n",
    "            t = t + 1\n",
    "            tp = t*0.1\n",
    "#             mid_layer = X_train[i].view(1,-1).mm(w1).clamp(min=0)\n",
    "            mid_layer = torch.tanh(X_train[i].view(1,-1).mm(w1))\n",
    "            out_layer = torch.sigmoid(mid_layer.mm(w2))\n",
    "            \n",
    "            eps1, eps2 = 1e-2,1e2\n",
    "            loss = (y_train[i] * (out_layer.clamp(eps1, eps2).log())) + \\\n",
    "                ((1-y_train[i]) * ((1-out_layer).clamp(eps1, eps2).log()))  \n",
    "            loss.backward()\n",
    " \n",
    "            lr1 = learning_rate * w1.grad.data\n",
    "            lr2 = learning_rate * w2.grad.data\n",
    "            \n",
    "            w1.data = w1.data + lr1\n",
    "            w2.data = w2.data + lr2\n",
    "            \n",
    "            w2.grad.data.zero_()\n",
    "            w1.grad.data.zero_()\n",
    "        print(loss.data)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.783185095\n"
     ]
    }
   ],
   "source": [
    "weights=(w1.data.numpy(),1,w2.data.numpy(),1)\n",
    "a=train_images\n",
    "b=train_labels\n",
    "d_train = predict_nn(train_images,\\\n",
    "            np.transpose(weights[0]),weights[1],\\\n",
    "            weights[2],weights[3])  \n",
    "ac_train=accuracy(d_train,train_labels)\n",
    "print(ac_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=5000\n",
    "hidden_layers=10\n",
    "output_nodes=1\n",
    "m=train_images.shape[1]\n",
    "X_train = Variable(torch.from_numpy(train_images)).double()\n",
    "y_train = Variable(torch.from_numpy(train_labels).view(-1,1)).double()\n",
    "\n",
    "def accuracy(y_real, y_predicted):\n",
    "    n = y_real.size()[0]\n",
    "    return (y_predicted.data==y_real.data).sum()/n\n",
    "\n",
    "def predict(x,w1,w2):\n",
    "    return torch.ge(torch.sigmoid(x.mm(w1).mm(w2)),0.5).double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | Dev\n",
      "0.621075\n"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "train_accuracy = []\n",
    "dev_accuracy = []\n",
    "\n",
    "test_set_accuracy = []\n",
    "epoch_points_train = []\n",
    "epoch_points_dev = []\n",
    "print(\"Train | Dev\")\n",
    "for super in range(1):\n",
    "    w1 = Variable((0.2*torch.randn(m,hidden_layers)).double(),requires_grad=True) # m x P\n",
    "    w2 = Variable((0.2*torch.randn(hidden_layers,output_nodes)).double(),requires_grad=True) # P x output\n",
    "    \n",
    "    m1_ = Variable((torch.zeros((m, hidden_layers)).double()))\n",
    "    v1 = Variable((torch.zeros((m, hidden_layers)).double()))\n",
    "    \n",
    "    m2_ = Variable((torch.zeros((hidden_layers, output_nodes)).double()))\n",
    "    v2 = Variable((torch.zeros((hidden_layers, output_nodes)).double()))\n",
    "    t = 0\n",
    "    for iterations in range(1):\n",
    "        for i in range(n):\n",
    "            \n",
    "            mid_layer = torch.sigmoid(X_train[i].view(1,-1).mm(w1))\n",
    "            out_layer = torch.sigmoid(mid_layer.mm(w2))\n",
    "            \n",
    "            eps1, eps2 = 1e-2,1e2\n",
    "            loss = (y_train[i] * (out_layer.clamp(eps1, eps2).log())) + \\\n",
    "                ((1-y_train[i]) * ((1-out_layer).clamp(eps1, eps2).log()))  \n",
    "            loss.backward()\n",
    " \n",
    "            lr1 = learning_rate * w1.grad.data\n",
    "            lr2 = learning_rate * w2.grad.data\n",
    "            \n",
    "            w1.data = w1.data + lr1\n",
    "            w2.data = w2.data + lr2\n",
    "            \n",
    "            w2.grad.data.zero_()\n",
    "            w1.grad.data.zero_()\n",
    "            \n",
    "\n",
    "            if i%5000 == 0:\n",
    "                # rand_train = torch.from_numpy(np.random.choice(40000, 1000, replace=False))\n",
    "                # rand_dev = torch.from_numpy(np.random.choice(5000, 1000, replace=False))\n",
    "                train_accuracy.append(accuracy(y_train[rand_train],predict(X_train[rand_train], w1, w2)))\n",
    "                # dev_accuracy.append(accuracy(y_dev[rand_dev], X_dev[rand_dev].mm(w.t())))\n",
    "                # dev_accuracy.append(accuracy(y_dev, predict(X_dev, w1, w2)))\n",
    "       \n",
    "    a=accuracy(y_train,predict(X_train, w1, w2))\n",
    "    print(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.793631185\n"
     ]
    }
   ],
   "source": [
    "weights=(w1.data.numpy(),1,w2.data.numpy(),1)\n",
    "a=train_images\n",
    "b=train_labels\n",
    "d_train = predict_nn(train_images,\\\n",
    "            np.transpose(weights[0]),weights[1],\\\n",
    "            weights[2],weights[3])\n",
    "    \n",
    "    \n",
    "ac_train=accuracy_2(d_train,train_labels)\n",
    "print(ac_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
